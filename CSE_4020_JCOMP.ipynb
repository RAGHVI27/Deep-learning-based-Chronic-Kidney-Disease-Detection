{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR3qdqe6WorLGF9PV0PJJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAGHVI27/Deep-learning-based-Chronic-Kidney-Disease-Detection/blob/main/CSE_4020_JCOMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4HS_GbT-x1B6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "df = pd.read_csv('kidney_disease_1.csv')\n",
        "#df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "# Data cleaning and preprocessing\n",
        "numerical_columns = ['age', 'bp', 'sg', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc']\n",
        "#df[numerical_columns] = df[numerical_columns].astype(float)\n",
        "#imputer = SimpleImputer(strategy='mean')\n",
        "#df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
        "# Impute missing values in numerical columns with mode\n",
        "numerical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n",
        "# Assuming df is your DataFrame\n",
        "df.replace('\\t?', np.nan, inplace=True)  # Replace '\\t?' with NaN\n",
        "\n",
        "for col in numerical_columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with the mode for categorical columns\n",
        "categorical_columns = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
        "for col in categorical_columns:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "ovsJN-atTdWU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features using Label Encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df.drop('classification', axis=1)\n",
        "y = df['classification']\n",
        "\n",
        "\n",
        "# Impute missing values\n",
        "X.fillna(X.mean(), inplace=True)  # You can use other imputation methods as well\n",
        "\n",
        "# Convert target variable to numeric\n",
        "y = pd.factorize(df['classification'])[0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\"\"\"\n",
        "# Feature selection using Random Forest\n",
        "X = df.drop('classification', axis=1)\n",
        "y = df['classification']\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#############################MODIFICATION REQUIRED\n",
        "\n",
        "# Feature selection using Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "sfm = SelectFromModel(rf, threshold=0.1)\n",
        "sfm.fit(X_train, y_train)\n",
        "X_train = sfm.transform(X_train)\n",
        "X_test = sfm.transform(X_test)"
      ],
      "metadata": {
        "id": "8c3B77dEPE_s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming df is your DataFrame with NaN values\n",
        "# Assuming 'classification' is the target variable, adjust accordingly\n",
        "\n",
        "# Drop the target variable and any other non-numeric columns if needed\n",
        "X = df.drop(['classification'], axis=1)\n",
        "\n",
        "# Impute missing values\n",
        "X.fillna(X.mean(), inplace=True)  # You can use other imputation methods as well\n",
        "\n",
        "# Convert target variable to numeric\n",
        "y = pd.factorize(df['classification'])[0]\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Fit the classifier\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame with feature names and their importances\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the feature importance DataFrame\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz_aH_yAe7M4",
        "outputId": "fe3fec7f-d5b9-4f8c-9c8b-6bd3f4086b1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature  Importance\n",
            "0       id    0.341896\n",
            "12      sc    0.115544\n",
            "16     pcv    0.111346\n",
            "3       sg    0.108398\n",
            "15    hemo    0.083960\n",
            "4       al    0.082725\n",
            "19     htn    0.024672\n",
            "18      rc    0.023718\n",
            "13     sod    0.021059\n",
            "11      bu    0.016115\n",
            "20      dm    0.014729\n",
            "10     bgr    0.014324\n",
            "17      wc    0.010580\n",
            "1      age    0.007730\n",
            "14     pot    0.005840\n",
            "2       bp    0.004091\n",
            "22   appet    0.004010\n",
            "23      pe    0.002048\n",
            "7       pc    0.002042\n",
            "5       su    0.001922\n",
            "24     ane    0.001270\n",
            "6      rbc    0.001254\n",
            "8      pcc    0.000584\n",
            "9       ba    0.000092\n",
            "21     cad    0.000053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape data for LSTM input\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Train the LSTM model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Get the output of the LSTM model\n",
        "lstm_output = model.layers[0](X_train)\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)  # Threshold for binary classification\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ghNC7vJ2Nf",
        "outputId": "7c3b1f12-f321-4446-e321-038af35b9fca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 4s 10ms/step - loss: 0.7393 - accuracy: 0.3313\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5123 - accuracy: 0.0063\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.3554 - accuracy: 0.0063\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.1869 - accuracy: 0.0063\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 15ms/step - loss: -0.1256 - accuracy: 0.0219\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 16ms/step - loss: -0.7356 - accuracy: 0.1750\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: -1.6772 - accuracy: 0.3125\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: -2.6321 - accuracy: 0.4375\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: -3.6969 - accuracy: 0.5156\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 9ms/step - loss: -4.2919 - accuracy: 0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c98d617ac20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        52\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.65        80\n",
            "   macro avg       0.33      0.33      0.33        80\n",
            "weighted avg       0.65      0.65      0.65        80\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaN values in y_pred:\", np.isnan(y_pred).any())\n",
        "print(\"Inf values in y_pred:\", np.isinf(y_pred).any())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkkzOKlUtyQT",
        "outputId": "3cbb8300-52a7-4989-d2b1-d240a2675ff4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in y_pred: False\n",
            "Inf values in y_pred: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "Xc18T18xyL6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "\n",
        "# GRU Model\n",
        "gru_model = Sequential()\n",
        "gru_model.add(GRU(64, input_shape=(lstm_output.shape[1], 1)))\n",
        "gru_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape data for GRU input\n",
        "lstm_output_reshaped = lstm_output.reshape(lstm_output.shape[0], lstm_output.shape[1], 1)\n",
        "\n",
        "# Train the GRU model\n",
        "gru_model.fit(lstm_output_reshaped, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Get the output of the GRU model\n",
        "gru_output = gru_model.layers[0](lstm_output_reshaped)\n",
        "\n",
        "# Evaluate the GRU model\n",
        "gru_predictions = gru_model.predict(lstm_output_reshaped)\n",
        "gru_predictions = (gru_predictions > 0.5)  # Threshold for binary classification\n",
        "\n",
        "# Print classification report for the GRU model\n",
        "print(classification_report(y_train, gru_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SViZ23tqvZh1",
        "outputId": "0b69d48e-55f2-462f-9eb0-4f12d7e9729a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 5s 49ms/step - loss: 0.6618 - accuracy: 0.0625\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.5772 - accuracy: 0.0063\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4316 - accuracy: 0.0063\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 0.1268 - accuracy: 0.0063\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 38ms/step - loss: -0.4075 - accuracy: 0.0063\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 37ms/step - loss: -1.2383 - accuracy: 0.0063\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 35ms/step - loss: -2.7263 - accuracy: 0.4875\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 37ms/step - loss: -3.5896 - accuracy: 0.5969\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 33ms/step - loss: -4.0986 - accuracy: 0.6062\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 33ms/step - loss: -4.5691 - accuracy: 0.6062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c98c5ba9120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 11ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       196\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00       122\n",
            "\n",
            "    accuracy                           0.60       320\n",
            "   macro avg       0.33      0.32      0.33       320\n",
            "weighted avg       0.61      0.60      0.60       320\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN\n",
        "\n",
        "# RNN Model\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(32, input_shape=(gru_output.shape[1], 1)))\n",
        "rnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape data for RNN input\n",
        "gru_output_reshaped = gru_output.reshape(gru_output.shape[0], gru_output.shape[1], 1)\n",
        "\n",
        "# Train the RNN model\n",
        "rnn_model.fit(gru_output_reshaped, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Get the output of the RNN model\n",
        "rnn_predictions = rnn_model.layers[0](gru_output_reshaped)\n",
        "\n",
        "# Evaluate the RNN model\n",
        "rnn_predictions = rnn_model.predict(gru_output_reshaped)\n",
        "rnn_predictions = (rnn_predictions > 0.5)  # Threshold for binary classification\n",
        "\n",
        "# Print classification report for the RNN model\n",
        "print(classification_report(y_train, rnn_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPJLjFMwedu",
        "outputId": "cca278ac-1584-428b-b5ad-b4291e9825f8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 2s 21ms/step - loss: -0.2006 - accuracy: 0.6062\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 16ms/step - loss: -0.8624 - accuracy: 0.6062\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 23ms/step - loss: -1.3212 - accuracy: 0.6062\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 27ms/step - loss: -1.6666 - accuracy: 0.6062\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 26ms/step - loss: -1.9635 - accuracy: 0.6062\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 35ms/step - loss: -2.2388 - accuracy: 0.6062\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 42ms/step - loss: -2.4763 - accuracy: 0.6094\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 18ms/step - loss: -2.7017 - accuracy: 0.6062\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 20ms/step - loss: -2.8950 - accuracy: 0.6062\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 16ms/step - loss: -3.1099 - accuracy: 0.6094\n",
            "10/10 [==============================] - 0s 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       196\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00       122\n",
            "\n",
            "    accuracy                           0.61       320\n",
            "   macro avg       0.33      0.33      0.33       320\n",
            "weighted avg       0.61      0.61      0.61       320\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # now we apply svm as meta learner\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get the output of the RNN model\n",
        "rnn_output = rnn_model.layers[0](gru_output)\n",
        "\n",
        "# Combine the RNN output with the original features\n",
        "X_train_combined = pd.concat([X_train, pd.DataFrame(rnn_output, columns=['rnn_output'])], axis=1)\n",
        "X_test_combined = pd.concat([X_test, pd.DataFrame(rnn_model.predict(X_test_gru_output), columns=['rnn_output'])], axis=1)\n",
        "\n",
        "# Define and train the SVM meta-learner\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_kernel = best_params['kernel']\n",
        "best_C = best_params['C']\n",
        "\n",
        "# Train the SVM classifier with the best hyperparameters\n",
        "best_svm = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
        "best_svm.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions with the best SVM classifier\n",
        "y_pred_best_svm = best_svm.predict(X_test_combined)\n",
        "\n",
        "# Print classification report for the final combined model with the best SVM\n",
        "print(\"Best Kernel:\", best_kernel)\n",
        "print(\"Best C:\", best_C)\n",
        "print(classification_report(y_test, y_pred_best_svm))\n",
        "#svm.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions with the SVM meta-learner\n",
        "#y_pred_svm = svm.predict(X_test_combined)\n",
        "\n",
        "# Print classification rport for the final combined model\n",
        "#print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "ySxoHT7jyWZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0ee7f8cf-20a4-4db4-8087-fcac83996068"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-15756a5740ad>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the output of the RNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Combine the RNN output with the original features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"simple_rnn_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (320, 64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "# Reshape X_train to 2D array\n",
        "num_samples, sequence_length, num_features = X_train.shape\n",
        "X_train_2d = X_train.reshape(num_samples, -1)\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}_time_{j}' for i in range(num_features) for j in range(sequence_length)]\n",
        "df_X_train = pd.DataFrame(X_train_2d, columns=columns)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_X_train)\n",
        "# Assuming rnn_predictions contains the output from the RNN model\n",
        "\n",
        "# Combine the RNN output with the original features\n",
        "#X_train_combined = pd.concat([X_train, pd.DataFrame(rnn_predictions, columns=['rnn_output'])], axis=1)\n",
        "#X_test_combined = pd.concat([X_test, pd.DataFrame(rnn_model.predict(X_test_rnn_output), columns=['rnn_output'])], axis=1)\n",
        "\n",
        "# Convert rnn_predictions to a DataFrame\n",
        "rnn_predictions_df = pd.DataFrame(rnn_predictions, columns=['rnn_output'])\n",
        "\n",
        "# Convert X_train to a DataFrame if it's not already\n",
        "#if not isinstance(X_train, pd.DataFrame):\n",
        "    #X_train = pd.DataFrame(X_train, columns=['feature1', 'feature2', ...])  # Replace with your column names\n",
        "#X_test_rnn_output = rnn_model.predict(X_test)\n",
        "# Combine the RNN output with the original features\n",
        "X_train_combined = pd.concat([df_X_train, rnn_predictions_df], axis=1)\n",
        "X_test_combined = pd.concat([X_test, pd.DataFrame(rnn_model.predict(rnn_predictions_df), columns=['rnn_output'])], axis=1)\n",
        "\n",
        "# Define and train the SVM meta-learner\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "svm = SVC(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_kernel = best_params['kernel']\n",
        "best_C = best_params['C']\n",
        "\n",
        "# Train the SVM classifier with the best hyperparameters\n",
        "best_svm = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
        "best_svm.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions with the best SVM classifier\n",
        "y_pred_best_svm = best_svm.predict(X_test_combined)\n",
        "\n",
        "# Print classification report for the final combined model with the best SVM\n",
        "print(\"Best Kernel:\", best_kernel)\n",
        "print(\"Best C:\", best_C)\n",
        "print(classification_report(y_test, y_pred_best_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "Jram_ZPSxyQO",
        "outputId": "db7d15c6-f71b-4e45-da53-2b9f1ea7d20b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     feature_0_time_0  feature_0_time_1  feature_0_time_2  feature_0_time_3  \\\n",
            "0                 3.0         -2.342354          4.000000          0.143536   \n",
            "1                18.0          1.342766          0.000000         -0.301054   \n",
            "2               202.0          0.421486          1.016949         -0.016517   \n",
            "3               250.0          1.342766          0.000000         -0.318837   \n",
            "4               274.0          0.421486          0.000000         -0.407755   \n",
            "..                ...               ...               ...               ...   \n",
            "315              71.0         -1.421074          1.000000          0.054618   \n",
            "316             106.0          0.421486          1.016949          0.552558   \n",
            "317             270.0          1.342766          0.000000         -0.336621   \n",
            "318             348.0          0.421486          0.000000         -0.443322   \n",
            "319             102.0         -1.421074          0.000000         -0.158785   \n",
            "\n",
            "     feature_0_time_4  \n",
            "0           -0.885839  \n",
            "1           -0.275390  \n",
            "2           -1.862557  \n",
            "3            1.067597  \n",
            "4            0.579238  \n",
            "..                ...  \n",
            "315         -1.374198  \n",
            "316         -2.717185  \n",
            "317          0.212969  \n",
            "318          0.579238  \n",
            "319          1.555955  \n",
            "\n",
            "[320 rows x 5 columns]\n",
            "10/10 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-aea7fac91918>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Combine the RNN output with the original features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mX_train_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_predictions_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mX_test_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_predictions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnn_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Define and train the SVM meta-learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;36m1\u001b[0m   \u001b[0;36m3\u001b[0m   \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 )\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSUMVSvk0LB9",
        "outputId": "dc14e530-29f5-4118-aeb7-22e08ff34323"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 3.00000000e+00]\n",
            "  [-2.34235440e+00]\n",
            "  [ 4.00000000e+00]\n",
            "  [ 1.43535723e-01]\n",
            "  [-8.85838942e-01]]\n",
            "\n",
            " [[ 1.80000000e+01]\n",
            "  [ 1.34276560e+00]\n",
            "  [ 0.00000000e+00]\n",
            "  [-3.01053786e-01]\n",
            "  [-2.75390345e-01]]\n",
            "\n",
            " [[ 2.02000000e+02]\n",
            "  [ 4.21485600e-01]\n",
            "  [ 1.01694915e+00]\n",
            "  [-1.65165002e-02]\n",
            "  [-1.86255670e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.70000000e+02]\n",
            "  [ 1.34276560e+00]\n",
            "  [ 0.00000000e+00]\n",
            "  [-3.36620946e-01]\n",
            "  [ 2.12968533e-01]]\n",
            "\n",
            " [[ 3.48000000e+02]\n",
            "  [ 4.21485600e-01]\n",
            "  [ 0.00000000e+00]\n",
            "  [-4.43322428e-01]\n",
            "  [ 5.79237692e-01]]\n",
            "\n",
            " [[ 1.02000000e+02]\n",
            "  [-1.42107440e+00]\n",
            "  [ 0.00000000e+00]\n",
            "  [-1.58785143e-01]\n",
            "  [ 1.55595545e+00]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reshape X_train to 2D array\n",
        "num_samples, sequence_length, num_features = X_train.shape\n",
        "X_train_2d = X_train.reshape(num_samples, -1)\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}_time_{j}' for i in range(num_features) for j in range(sequence_length)]\n",
        "df_X_train = pd.DataFrame(X_train_2d, columns=columns)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_X_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1yHMWOw1Jaq",
        "outputId": "e02d224a-574e-4138-ca46-38ba86573b6a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     feature_0_time_0  feature_0_time_1  feature_0_time_2  feature_0_time_3  \\\n",
            "0                 3.0         -2.342354          4.000000          0.143536   \n",
            "1                18.0          1.342766          0.000000         -0.301054   \n",
            "2               202.0          0.421486          1.016949         -0.016517   \n",
            "3               250.0          1.342766          0.000000         -0.318837   \n",
            "4               274.0          0.421486          0.000000         -0.407755   \n",
            "..                ...               ...               ...               ...   \n",
            "315              71.0         -1.421074          1.000000          0.054618   \n",
            "316             106.0          0.421486          1.016949          0.552558   \n",
            "317             270.0          1.342766          0.000000         -0.336621   \n",
            "318             348.0          0.421486          0.000000         -0.443322   \n",
            "319             102.0         -1.421074          0.000000         -0.158785   \n",
            "\n",
            "     feature_0_time_4  \n",
            "0           -0.885839  \n",
            "1           -0.275390  \n",
            "2           -1.862557  \n",
            "3            1.067597  \n",
            "4            0.579238  \n",
            "..                ...  \n",
            "315         -1.374198  \n",
            "316         -2.717185  \n",
            "317          0.212969  \n",
            "318          0.579238  \n",
            "319          1.555955  \n",
            "\n",
            "[320 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X_train to 2D array\n",
        "num_samples, sequence_length, num_features = X_train.shape\n",
        "X_train_2d = X_train.reshape(num_samples, -1)\n",
        "# Create a DataFrame\n",
        "columns = [f'feature_{i}_time_{j}' for i in range(num_features) for j in range(sequence_length)]\n",
        "df_X_train = pd.DataFrame(X_train_2d, columns=columns)\n",
        "# Convert rnn_predictions to a DataFrame\n",
        "rnn_predictions_df = pd.DataFrame(rnn_predictions, columns=['rnn_output'])\n",
        "\n",
        "# Combine the RNN output with the original features\n",
        "X_train_combined = pd.concat([df_X_train, rnn_predictions_df], axis=1)\n",
        "# Convert rnn_model.predict(X_test_rnn_output) to a DataFrame\n",
        "rnn_test_predictions_df = pd.DataFrame(rnn_model.predict(X_test_rnn_output), columns=['rnn_output'])\n",
        "\n",
        "# Combine the RNN output with the original features in the test set\n",
        "X_test_combined = pd.concat([X_test, rnn_test_predictions_df], axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "U86SSKYb1JUK",
        "outputId": "7c54b0fe-c1f4-4359-cadb-5e26380cceb8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-cf184f501b38>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX_train_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_predictions_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Convert rnn_model.predict(X_test_rnn_output) to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrnn_test_predictions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_rnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rnn_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Combine the RNN output with the original features in the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\", line 662, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'simple_rnn_1' (type SimpleRNN).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'simple_rnn_1' (type SimpleRNN):\n       inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n       mask=None\n       training=False\n       initial_state=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_rnn_output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zYXKUqj6EcL",
        "outputId": "8ef1cfc2-df9e-49c0-89b3-0b61927fa429"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Reshape X_test_rnn_output to have the correct shape\n",
        "X_test_rnn_output_reshaped = X_test_rnn_output.reshape((num_samples, sequence_length, num_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "KnxUyY5n6Gxu",
        "outputId": "9087b3e0-8f4c-4c73-f8c1-6b2dfbbbfc30"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-24af7225be8a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example: Reshape X_test_rnn_output to have the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test_rnn_output_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_rnn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 80 into shape (320,5,1)"
          ]
        }
      ]
    }
  ]
}